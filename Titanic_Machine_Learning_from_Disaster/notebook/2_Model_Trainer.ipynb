{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Trainer - Titanic (Machine Learning from Disaster)**\n",
    "\n",
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    root_dir: str = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Assuming the script is inside the notebook folder\n",
    "    artifacts_dir: str = os.path.join(root_dir, 'artifacts')\n",
    "    train_data_path: str=os.path.join(artifacts_dir,'train.csv')\n",
    "    test_data_path: str=os.path.join(artifacts_dir,'test.csv')\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self):\n",
    "        self.ingestion_config=DataIngestionConfig()\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "        try:\n",
    "            train_data=pd.read_csv('D:/Projects/KaggalProjects/Titanic_Machine_Learning_from_Disaster/notebook/train.csv')\n",
    "            test_data=pd.read_csv('D:/Projects/KaggalProjects/Titanic_Machine_Learning_from_Disaster/notebook/test.csv')\n",
    "\n",
    "            os.makedirs(self.ingestion_config.artifacts_dir,exist_ok=True)\n",
    "\n",
    "            train_data.to_csv(self.ingestion_config.train_data_path,index=False,header=True)\n",
    "            test_data.to_csv(self.ingestion_config.test_data_path,index=False,header=True)\n",
    "\n",
    "            return train_data,test_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print('The error is: ',e)\n",
    "\n",
    "obj=DataIngestion()\n",
    "titanic_train_data,titanic_test_data=obj.initiate_data_ingestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning and Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def data_imputing(self,train_data,test_data):\n",
    "        '''\n",
    "        This function is used to impute the missing values.\n",
    "        - In the training data, the Embarked column has two missing values and will be replaced with the most occuring embarked value.\n",
    "        - There is one missing value in Fare column of test data and will be replaced with the mode of the respective pClass.\n",
    "        - There are some missing values in Age feature of both training and testing and will be replaced with the mean of the Age column.\n",
    "        '''\n",
    "        try:\n",
    "            imputer=SimpleImputer(strategy='mean')\n",
    "            \n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "                                                                    #Training Part\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            mode_value = train_data['Embarked'].mode()[0]  # Calculate the mode of the column\n",
    "            train_data['Embarked'].fillna(mode_value, inplace=True) \n",
    "            train_data['Age']=imputer.fit_transform(train_data[['Age']])\n",
    "            \n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "                                                                    #Testing Part\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "            \n",
    "            missing_index = test_data['Fare'].isnull()\n",
    "            for idx in test_data[missing_index].index:\n",
    "                pclass_value = test_data.loc[idx, 'Pclass']\n",
    "                mode_fare = test_data[test_data['Pclass'] == pclass_value]['Fare'].mode()[0]\n",
    "                test_data.loc[idx, 'Fare'] = mode_fare\n",
    "            \n",
    "            test_data['Age']=imputer.fit_transform(test_data[['Age']])\n",
    "\n",
    "            return train_data,test_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('The error is: ',e)\n",
    "    \n",
    "    def feature_encoder(self,train_data,test_data):\n",
    "        '''\n",
    "        This function is used to give OneHotEncoder values to categorical columns.\n",
    "        - Categorical columns like Embarked and Sex are encoded using OneHotEncoder.\n",
    "        '''\n",
    "        try:\n",
    "            encoder=OneHotEncoder(sparse_output=False)\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "                                                                    #Training Part\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "            \n",
    "            encoded_embarked_train=encoder.fit_transform(train_data[[\"Embarked\"]])\n",
    "            encoded_col_names = ['Embarked_' + str(cat) for cat in encoder.categories_[0]]\n",
    "            encoded_embarked_df_train = pd.DataFrame(encoded_embarked_train,columns=encoded_col_names,index=train_data.index)\n",
    "            train_data = train_data.join(encoded_embarked_df_train)\n",
    "\n",
    "            encoded_sex_train=encoder.fit_transform(train_data[[\"Sex\"]])\n",
    "            encoded_col_names = ['Sex_' + str(cat) for cat in encoder.categories_[0]]\n",
    "            encoded_sex_df_train = pd.DataFrame(encoded_sex_train,columns=encoded_col_names,index=train_data.index)\n",
    "            train_data = train_data.join(encoded_sex_df_train)\n",
    "            \n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "                                                                    #Testing Part\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            encoded_embarked_test=encoder.fit_transform(test_data[[\"Embarked\"]])\n",
    "            encoded_col_names = ['Embarked_' + str(cat) for cat in encoder.categories_[0]]\n",
    "            encoded_embarked_df_test = pd.DataFrame(encoded_embarked_test,columns=encoded_col_names,index=test_data.index)\n",
    "            test_data = test_data.join(encoded_embarked_df_test)\n",
    "\n",
    "            encoded_sex_test=encoder.fit_transform(test_data[[\"Sex\"]])\n",
    "            encoded_col_names = ['Sex_' + str(cat) for cat in encoder.categories_[0]]\n",
    "            encoded_sex_df_test = pd.DataFrame(encoded_sex_test,columns=encoded_col_names,index=test_data.index)\n",
    "            test_data = test_data.join(encoded_sex_df_test)\n",
    "\n",
    "            return train_data,test_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('The error is: ',e)\n",
    "    \n",
    "    def feature_dropper(self,train_data,test_data):\n",
    "        '''\n",
    "        This function is used to drop few features from the dataframe.\n",
    "        - Features like Name, Sex, Cabin, Embarked, Ticket are dropped from the dataframe.\n",
    "        '''\n",
    "        try:\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "                                                                    #Training Part\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            train_data=train_data.drop([\"Name\", \"Sex\", \"Cabin\", \"Embarked\", \"Ticket\"],axis=1)\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "                                                                    #Testing Part\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            test_data=test_data.drop([\"Name\", \"Sex\", \"Cabin\", \"Embarked\", \"Ticket\"],axis=1)\n",
    "\n",
    "            return train_data,test_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('The error is: ',e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smits\\AppData\\Local\\Temp\\ipykernel_13976\\3507180048.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['Embarked'].fillna(mode_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data_impute=DataTransformation()\n",
    "impute_train_data,impute_test_data=data_impute.data_imputing(titanic_train_data,titanic_test_data)\n",
    "encoded_train_data,encoded_test_data=data_impute.feature_encoder(impute_train_data,impute_test_data)\n",
    "train_data,test_data=data_impute.feature_dropper(encoded_train_data,encoded_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(true, predicted):\n",
    "    try:\n",
    "        mae = mean_absolute_error(true, predicted)\n",
    "        mse = mean_squared_error(true, predicted)\n",
    "        rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "        r2_square = r2_score(true, predicted)\n",
    "        acc=accuracy_score(true, predicted)\n",
    "        precision=precision_score(true, predicted)\n",
    "        recall=recall_score(true, predicted)\n",
    "        f1=f1_score(true, predicted)\n",
    "\n",
    "        return mae, rmse, r2_square,acc,precision,recall,f1\n",
    "\n",
    "    except Exception as e:\n",
    "        print('The error is: ',e)\n",
    "\n",
    "def print_evaluated_models(xtrain,xtest,ytrain,ytest,models_list):\n",
    "    # try:\n",
    "    model_list = []\n",
    "    r2_list =[]\n",
    "\n",
    "    for model_name, model in models_list.items():\n",
    "        if model_name == 'Neural Network':\n",
    "            model.fit(xtrain, ytrain, epochs=50, batch_size=32, verbose=0)\n",
    "            y_train_pred = (model.predict(xtrain) > 0.5).astype(\"int32\")\n",
    "            y_test_pred = (model.predict(xtest) > 0.5).astype(\"int32\")\n",
    "\n",
    "        else:\n",
    "            model.fit(xtrain, ytrain.values.flatten()) # Train model\n",
    "            y_train_pred = model.predict(xtrain)\n",
    "            y_test_pred = model.predict(xtest)\n",
    "        \n",
    "        # Evaluate Train and Test dataset\n",
    "        model_train_mae , model_train_rmse, model_train_r2,model_train_acc,model_train_precision,model_train_recall,model_train_f1 = model_metrics(ytrain, y_train_pred)\n",
    "        model_test_mae , model_test_rmse, model_test_r2,model_test_acc,model_test_precision,model_test_recall,model_test_f1 = model_metrics(ytest, y_test_pred)\n",
    "\n",
    "        print(f\"\\033[1m{model_name}\\033[0m\")\n",
    "        model_list.append(model_name)\n",
    "        \n",
    "        print('Model performance for Training set')\n",
    "        print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n",
    "        print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n",
    "        print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n",
    "        print(\"- Accuracy: {:.4f}\".format(model_train_acc))\n",
    "        print(\"- Precision: {:.4f}\".format(model_train_precision))\n",
    "        print(\"- Recall: {:.4f}\".format(model_train_recall))\n",
    "        print(\"- F1 Score: {:.4f}\".format(model_train_f1))\n",
    "\n",
    "        print('----------------------------------')\n",
    "        \n",
    "        print('Model performance for Validation set')\n",
    "        print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n",
    "        print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n",
    "        print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n",
    "        print(\"- Accuracy: {:.4f}\".format(model_test_acc))\n",
    "        print(\"- Precision: {:.4f}\".format(model_test_precision))\n",
    "        print(\"- Recall: {:.4f}\".format(model_test_recall))\n",
    "        print(\"- F1 Score: {:.4f}\".format(model_test_f1))\n",
    "        r2_list.append(model_test_r2)\n",
    "        \n",
    "        print('='*35)\n",
    "        print('\\n')\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print('The error is: ',e)\n",
    "\n",
    "def save_model(model,model_name):\n",
    "    '''\n",
    "    This function is used to save the best model.\n",
    "    '''\n",
    "    try:\n",
    "        root_dir: str = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Assuming the script is inside the notebook folder\n",
    "        model_dir: str = os.path.join(root_dir, 'model')\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        # Construct the full path\n",
    "        file_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "        joblib.dump(model, file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('The error is: ',e)\n",
    "\n",
    "def load_model(model_path):\n",
    "    '''\n",
    "    This function is used to load the model.\n",
    "    '''\n",
    "    try:\n",
    "        model=joblib.load(model_path)\n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('The error is: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_data.drop(\"Survived\",axis=1)\n",
    "y=train_data[\"Survived\"]\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\KaggalProjects\\Titanic_Machine_Learning_from_Disaster\\titanicvenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "d:\\Projects\\KaggalProjects\\Titanic_Machine_Learning_from_Disaster\\titanicvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m create_neural_network()\n\u001b[0;32m     21\u001b[0m models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeural Network\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m nn_model\n\u001b[1;32m---> 23\u001b[0m \u001b[43mprint_evaluated_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mprint_evaluated_models\u001b[1;34m(xtrain, xtest, ytrain, ytest, models_list)\u001b[0m\n\u001b[0;32m     34\u001b[0m model_train_mae , model_train_rmse, model_train_r2,model_train_acc,model_train_precision,model_train_recall,model_train_f1 \u001b[38;5;241m=\u001b[39m model_metrics(ytrain, y_train_pred)\n\u001b[0;32m     35\u001b[0m model_test_mae , model_test_rmse, model_test_r2,model_test_acc,model_test_precision,model_test_recall,model_test_f1 \u001b[38;5;241m=\u001b[39m model_metrics(ytest, y_test_pred)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(models_list\u001b[38;5;241m.\u001b[39mkeys())[\u001b[43mi\u001b[49m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m model_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(models_list\u001b[38;5;241m.\u001b[39mkeys())[i])\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel performance for Training set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "# Neural Network Model\n",
    "def create_neural_network():\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    # 'Decision Tree': DecisionTreeClassifier(),\n",
    "    # 'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    # 'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    # 'Support Vector Machine': SVC(),\n",
    "    # 'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "nn_model = create_neural_network()\n",
    "models['Neural Network'] = nn_model\n",
    "\n",
    "print_evaluated_models(X_train,X_val,y_train,y_val,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanicvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
